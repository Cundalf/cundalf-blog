
---
title: Gitlab Runners, AWS Spot, Socotrocos
description: Una de las cosas lindas que me toco hacer es que los runners de Gitlab utilicen instancias Spot, te cuento...
slug: aws-spot-runners
date: 2025-11-11 00:00:00+0000
image: cover.jpg
tags:
    - devops
categories:
    - devops
---

Para que me puedan seguir vamos a tener que alinearnos en algunas definiciones y aclarar algunos puntos...

## Definiciones

- **AWS**: Amazon Web Services ‚Äî el conjunto de servicios en la nube de Amazon. Incluye servidores, almacenamiento y redes que puedes usar por demanda sin comprar hardware.

- **AWS EC2 (Elastic Compute Cloud)**: servicio para crear y ejecutar m√°quinas virtuales (instancias). Aqu√≠ instalas sistemas operativos, software y ejecutas cargas de trabajo.

- **CI/CD**: Integraci√≥n Continua y Entrega/Despliegue Continuo.  
    - CI: cada cambio de c√≥digo se compila y se prueba autom√°ticamente.  
    - CD: una vez que pasa las pruebas, el artefacto se entrega o despliega autom√°ticamente a entornos (staging/producci√≥n).

- **Runner (GitLab Runner)**: servicio que ejecuta los jobs de CI/CD (compilar, testear, empaquetar, desplegar). GitLab env√≠a jobs al runner y este los ejecuta dentro de contenedores Docker, aislando todo para que no se rompan cosas.

- **Runner Manager / Coordinador**: servicio que orquesta todo el circo de runners din√°micos en la nube. Escucha a GitLab, ve qu√© onda con los recursos disponibles y crea o destruye runners seg√∫n lo que necesite en cada momento.

- **Plugin Fleeting (`fleeting-plugin-aws`)**: el tel√©fono del Runner Manager para hablar con AWS. Le permite solicitar m√°quinas nuevas, monitorearlas y terminarlas cuando se acab√≥ el laburo. Automatiza todo ese circo para que el Runner Manager no tenga que llorar pidiendo m√°quinas a mano.

- **EC2 Spot Instance**: instancia EC2 con un descuento importante respecto al precio bajo demanda. A cambio, AWS puede terminarla con poco aviso si necesita recuperar capacidad. Ideal para trabajos tolerantes a interrupciones y para ahorrar dinero.

- **ASG (Auto Scaling Group)**: grupo que gestiona una flota de instancias EC2: crea, elimina y reemplaza instancias seg√∫n pol√≠ticas de escalado y chequeos de salud. √ötil para mantener disponibilidad y ajustar capacidad autom√°ticamente.

## Aclaraciones

Voy a describir los resultados de varias iteraciones: qu√© se implement√≥ y qu√© problemas solucion√≥. No pretende ser la √∫nica ni la mejor forma de hacerlo; existen muchas alternativas y esta experiencia est√° ligada a un contexto que no puedo compartir por completo. T√≥menlo como referencia, no como una receta definitiva.

## Alabado sea el Omnissiah

Frase que he repetido mucho en este proceso, aunque los que no saben sobre Warhammer 40k se me queden mirando y pensando "Uh, est√° gaga". El que ha levantado infraestructuras completas, ha renegado con una migraci√≥n, ha realizado tareas de devops o todas a la vez, entiende esa satisfacci√≥n de ver todo funcionando y pensando en lo siguiente a refactorizar o meter mano cuando ya andaba bien como un desquiciado.

Como esta hermosa imagen que me cruce alguna vez y nunca pude olvidar.

![big plans](bigplans.jpg)

### Contexto

Vamos con contexto. Los runners corr√≠an en un servidor compartido con otros servicios. Recursos finitos. Proyectos creciendo. Pipelines cada vez m√°s lentos. El resultado: p√©rdida de tiempo y dinero.

La cuesti√≥n empieza ac√°, donde ampliar hardware implica dinero. Y la pregunta es ¬øc√≥mo ampliamos, horizontalmente o verticalmente?

Para el que nunca escuch√≥ esto... Ya ya, Cundita explica todo. En simples palabras, ampliar verticalmente es al servidor que ya tengo, meterle m√°s fierro (bien criollo el t√©rmino, ¬øvio'?). Crecer horizontalmente es poner un servidor al lado y que se pongan t√≠midos.

![servers](servers.jpg)

### ¬øOpciones?

Para no hacerlos perder tiempo: si ampliamos el hardware, solucionamos el problema temporalmente. Pero escalar no es barato, y las opciones simples (misma instancia con m√°s potencia) no justificaban la inversi√≥n. Poner dos servidores tampoco era la soluci√≥n: ahora ten√≠as que mantener dos m√°quinas en lugar de una.

Para explicarlo con una analog√≠a de Warhammer -hoy estoy a full, hace rato no tengo tiempo para viciar, sepan entender- 

En ese momento era como Tech-Priest del Adeptus Mechanicus y necesitaba potencia de c√°lculo para simular patrones de disformidad. Mis opciones eran:

üü• Opci√≥n 1: Servidor On-Demand
Es como tener tu propio [Cogitador](https://warhammer40k.fandom.com/es/wiki/Cogitador) de N√∫cleo Dedicado encendido todo el d√≠a en tu taller.
- Siempre disponible.
- Confiable.
- Pero consume recursos sagrados constantemente (muchos cr√©ditos imperiales).
- Ideal para tareas cr√≠ticas o persistentes.

---

üü© Opci√≥n 2: Instancia Spot con mucho hardware
Es como pedir [servitors](https://warhammer40k.fandom.com/wiki/Servitor) de mundos forja.
- Est√°n ociosos, esperando tareas.
- Son baratos, porque nadie m√°s los est√° usando.
- Los "pido" con un solo prop√≥sito (en este caso).
- Vienen con potencia bruta: CPU, RAM, GPU, lo que necesites.
- Pero si el Omnissiah los reclama para otra misi√≥n, desaparecen sin aviso (raspando la referencia, JA).

![tech](tech.png)

### Contexto - Presupuesto

Sin entrar en detalle del porqu√©, el presupuesto m√°ximo era gastar 100 d√≥lares por mes. Con las opciones cl√°sicas de escalamiento, nos pas√°bamos del presupuesto sin ni siquiera resolver el problema.

Ac√° apareci√≥ la magia de correr los jobs en una Instancia Spot.

SPOILER: terminamos alquilando el triple de hardware que las instancias que usamos, dedicado solo a los jobs gastando menos de 20 dolares mensuales.

## Ahora s√≠... el c√≥mo

La idea principal es entender que un runner ejecuta los jobs de tu pipeline. Dependiendo del contexto, un runner puede procesar m√∫ltiples jobs en simult√°neo, o pod√©s tener varios runners trabajando en paralelo. Todo depende de lo que necesites.

Voy a explicar esto con analog√≠as (sin warhammer esta vez, al menos lo intentar√©).

### Runners Manager con Fleeting y ASG

Imaginemos que tenemos una **f√°brica de trabajos** (los pipelines de GitLab). Algunos d√≠as tenemos montones de trabajo, otros d√≠as casi nada. No tiene sentido tener m√°quinas costosas funcionando 24/7 si solo las necesitamos a veces. Sin mencionar que las m√°quinas que podemos tener 24/7 nos est√°n quedando "chicas".

Por eso existe este sistema: **m√°quinas que aparecen cuando las necesitas y desaparecen cuando no las usas** (por si no se entendi√≥ c√≥mo es una m√°quina spot con mi referencia a Warhammer). Es como contratar trabajadores temporales en lugar de empleados fijos.

Ahora bien, veamos c√≥mo funciona todo esto en la pr√°ctica.

#### Los personajes principales

##### 1. **El Runner Manager** (el capataz)

Es un contenedor Docker que corre en tu servidor principal. Su √∫nico trabajo es:

- Escuchar a GitLab: "Che, tengo un job nuevo que ejecutar"
- Mirar el ASG: "¬øCu√°ntas m√°quinas tenemos disponibles?"
- Decidir: "Necesito m√°s m√°quinas, voy a crearlas"
- Distribuir trabajo: "Vos, la m√°quina nueva, ejecuta este job"

> **Nota:** Los jobs se ejecutan dentro de contenedores Docker en cada m√°quina Spot. Esto permite aislar dependencias, versiones de software y ambiente de ejecuci√≥n para cada job. En nuestro caso decidimos meterlo todo en Docker; en otros contextos puede ser diferente.

##### 2. **El Plugin Fleeting** (el asistente del capataz)

Es una extensi√≥n que le da superpoderes al Runner Manager. Le permite:

- Hablar con AWS
- Decirle: "Crea 5 m√°quinas nuevas, por favor"
- Decirle: "Esas m√°quinas que creamos hace 10 minutos, ya no las necesitamos, elim√≠nalas"

##### 3. **El ASG** (Auto Scaling Group) - El almac√©n de m√°quinas

Es un servicio de AWS que:

- Guarda la "receta" de c√≥mo crear m√°quinas (qu√© tipo, qu√© sistema operativo, etc.)
- Mantiene un grupo de m√°quinas listas
- Puede crear y eliminar m√°quinas autom√°ticamente

##### 4. **Las Spot Instances** (empleados temporales baratos)

Son m√°quinas EC2 normales pero con un descuento BRUTAL -en una charla dije que son los "flybondi" de las aerol√≠neas-. El √∫nico problema: AWS puede decir "necesito esta m√°quina, adi√≥s" sin avisar.

#### **Paso a paso desde que ejecutas un pipeline:**

```
1Ô∏è‚É£  PIPELINE INICIADO
    ‚îî‚îÄ Desarrollador hace push a GitLab
    ‚îî‚îÄ GitLab crea un "job" (una tarea)
    ‚îî‚îÄ GitLab dice: "Runner Manager, tengo un job para ti"

2Ô∏è‚É£  EL RUNNER MANAGER ESCUCHA
    ‚îî‚îÄ "Hola GitLab, estoy aqu√≠. Dame el job"
    ‚îî‚îÄ GitLab describe qu√© necesita (compilar, test, desplegar, etc.)
    ‚îî‚îÄ Runner Manager pregunta: "¬øTengo m√°quinas disponibles?"

3Ô∏è‚É£  DECISI√ìN: ¬øM√°quinas disponibles?
    
    SI HAY M√ÅQUINAS LIBRES ‚Üí Usa una de inmediato
    ‚îÇ
    NO HAY ‚Üí El Plugin Fleeting entra en acci√≥n:
    ‚îÇ       ‚îú‚îÄ "AWS, dame m√°quinas nuevas por favor"
    ‚îÇ       ‚îú‚îÄ ASG dice: "Voy a crear 3 m√°quinas Spot"
    ‚îÇ       ‚îú‚îÄ AWS lanza las m√°quinas (30-60 segundos)
    ‚îÇ       ‚îî‚îÄ Las m√°quinas se registran con el Runner Manager
    ‚îÇ
    ‚îî‚îÄ‚Üí El Runner Manager le asigna el job a una m√°quina

4Ô∏è‚É£  M√ÅQUINA EJECUTA EL JOB
    ‚îî‚îÄ La m√°quina EC2 descarga el c√≥digo
    ‚îî‚îÄ Corre los tests, compilaciones, despliegues, etc.
    ‚îî‚îÄ Env√≠a los resultados a GitLab
    ‚îî‚îÄ ‚úÖ Job terminado

5Ô∏è‚É£  LIMPIEZA (ahorro de dinero)
    ‚îî‚îÄ La m√°quina queda "idle" (sin hacer nada)
    ‚îî‚îÄ Runner Manager cuenta el tiempo: 
    ‚îÇ   ‚îî‚îÄ En nuestro contexto, en horario laboral las m√°quinas tienen mayor tiempo en IDLE que fuera de horario laboral. As√≠ ahorramos tiempos y dinero.
    ‚îÇ
    ‚îî‚îÄ Se acab√≥ el tiempo ‚Üí "AWS, elimina esa m√°quina"
    ‚îî‚îÄ La m√°quina desaparece
    ‚îî‚îÄ üí∞ Dejas de pagar por ella
```


#### Resumen en 3 puntos

1. **Runner Manager = Director de orquesta**: Escucha a GitLab, ve si hay m√°quinas, y decide qu√© hacer.

2. **Fleeting Plugin = Tel√©fono a AWS**: Le dice a AWS cu√°ndo crear y eliminar m√°quinas.

3. **ASG = Banco de m√°quinas**: Guarda la receta y crea/elimina m√°quinas cuando se lo piden.

**Resultado: M√°quinas que aparecen cuando las necesitas, desaparecen cuando no, y ahorras mucho dinero.** üöÄ


## Para cerrar

Implementar esto no fue trivial ni barato, pero vali√≥ totalmente la pena. Pasamos de renegar todos los d√≠as con runners saturados y sufrir los cuellos de botella a un sistema que **escala autom√°ticamente seg√∫n la demanda real**.

> **Y lo mejor:** gastamos **menos de $20 mensuales** en lugar de los **$100+** que costaba mantener la infra anterior.

### El aprendizaje m√°s importante

La gran lecci√≥n de todo esto fue entender que **no siempre necesitas lo mejor**, sino **lo justo en el momento justo**. Y eso es lo hermoso de la nube cuando la aprovechas bien. No se trata de tener la m√°quina m√°s poderosa corriendo 24/7, sino de tener exactamente lo que necesitas, cuando lo necesitas.

### Seguimos iterando

Claro que hay oportunidades de mejora. Hay m√°s soluciones en la nube para explotar, pero de eso se trata: **iterar**. As√≠ como antes pasamos de servers chicos a VMs en la nube, optimizamos los pipelines conforme crec√≠an, y le dimos distintas misiones a runners diferentes para aprovechar mejor los recursos... ahora toca seguir puliendo.

La infraestructura es una obra viva, no un producto terminado. üîß

Y obviamente... verlo funcionar en vivo y en tiempo real es esa satisfacci√≥n que solo se puede expresar as√≠: "ALABADO SEA EL OMNISSIAH" üôè

![alabado](alabado.jpg)